{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw6_Gan","provenance":[{"file_id":"https://github.com/ga642381/ML2021-Spring/blob/main/HW06/HW06.ipynb","timestamp":1619355932001}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"oZ-C2Dgetg37"},"source":["# **Homework 6 - Generative Adversarial Network**\n","\n","This is the example code of homework 6 of the machine learning course by Prof. Hung-yi Lee.\n","\n","\n","In this homework, you are required to build a generative adversarial  network for anime face generation.\n"]},{"cell_type":"markdown","metadata":{"id":"JTBkY5QFf3QM"},"source":["## Set up the environment\n"]},{"cell_type":"markdown","metadata":{"id":"Y7y4wyYdEABR"},"source":["### Packages Installation"]},{"cell_type":"code","metadata":{"id":"6IQB485dD_eL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619438654539,"user_tz":-480,"elapsed":5098,"user":{"displayName":"劉德駿","photoUrl":"","userId":"10755332246774080837"}},"outputId":"ea88709f-22c4-424b-c188-fa84a594f51c"},"source":["# You may replace the workspace directory if you want.\n","workspace_dir = '.'\n","\n","# Training progress bar\n","!pip install -q qqdm"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  Building wheel for qqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2NFjuZTPDxLn"},"source":["### Download the dataset\n","**Please use the link according to the last digit of your student ID first!**\n","\n","If all download links fail, please follow [here](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e).\n","\n","* To open the file using your browser, use the link below (replace the id first!):\n","https://drive.google.com/file/d/REPLACE_WITH_ID\n","* e.g. https://drive.google.com/file/d/1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p"]},{"cell_type":"code","metadata":{"id":"uZomvVA2f607","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619438711340,"user_tz":-480,"elapsed":6025,"user":{"displayName":"劉德駿","photoUrl":"","userId":"10755332246774080837"}},"outputId":"4a1d2e9c-bc0f-4fa1-a048-98e892d66439"},"source":["!gdown --id 1IGrTr308mGAaCKotpkkm8wTKlWs9Jq-p --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# Other download links\n","#   Please uncomment the line according to the last digit of your student ID first\n","\n","# 0\n","# !gdown --id 131zPaVoi-U--XThvzgRfaxrumc3YSBd3 --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 1\n","# !gdown --id 1kCuIj1Pf3T2O94H9bUBxjPBKb---WOmH --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 2\n","# !gdown --id 1boEoiiqBJwoHVvjmI0xgoutE9G0Rv8CD --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 3\n","#!gdown --id 1Ic0mktAQQvnNAnswrPHsg-u2OWGBXTWF --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 4\n","# !gdown --id 1PFcc25r9tLE7OyQ-CDadtysNdWizk6Yg --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 5\n","# !gdown --id 1wgkrYkTrhwDSMdWa5NwpXeE4-7JaUuX2 --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 6\n","# !gdown --id 19gwNYWi9gN9xVL86jC3v8qqNtrXyq5Bf --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 7 \n","# !gdown --id 1-KPZB6frRSRLRAtQfafKCVA7em0_NrJG --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 8\n","# !gdown --id 1rNBfmn0YBzXuG5ub7CXbsGwduZqEs8hx --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 9\n","# !gdown --id 113NEISX-2j6rBd1yyBx0c3_9nPIzSNz- --output \"{workspace_dir}/crypko_data.zip\"\n","\n","# 我自己的dropbox\n","# !wget 'https://www.dropbox.com/s/h36i9a1jsurlmjj/crypko_data_divers2.zip.zip%20%E7%9A%84%E5%89%AF%E6%9C%AC?dl=0' -O \"{workspace_dir}/crypko_data.zip\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-04-26 12:05:05--  https://www.dropbox.com/s/h36i9a1jsurlmjj/crypko_data_divers2.zip.zip%20%E7%9A%84%E5%89%AF%E6%9C%AC?dl=0\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6017:18::a27d:212\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: /s/raw/h36i9a1jsurlmjj/crypko_data_divers2.zip.zip%20%E7%9A%84%E5%89%AF%E6%9C%AC [following]\n","--2021-04-26 12:05:06--  https://www.dropbox.com/s/raw/h36i9a1jsurlmjj/crypko_data_divers2.zip.zip%20%E7%9A%84%E5%89%AF%E6%9C%AC\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uc3411270331445d0c4fb2c3b724.dl.dropboxusercontent.com/cd/0/inline/BNVp5rwe8Nivz-HyfUolsbhVDcGjdG0sJFuZDZOEuHf0Ez95HfyxWjJEosfIpD91hNHw4Lsd_dy268HEFiQUYapgPWzTTdJS25gsb8kUeodIcigCDlidLMLvpJ8U74cOED_lCqq0_sjvfWabJaz4uomY/file# [following]\n","--2021-04-26 12:05:06--  https://uc3411270331445d0c4fb2c3b724.dl.dropboxusercontent.com/cd/0/inline/BNVp5rwe8Nivz-HyfUolsbhVDcGjdG0sJFuZDZOEuHf0Ez95HfyxWjJEosfIpD91hNHw4Lsd_dy268HEFiQUYapgPWzTTdJS25gsb8kUeodIcigCDlidLMLvpJ8U74cOED_lCqq0_sjvfWabJaz4uomY/file\n","Resolving uc3411270331445d0c4fb2c3b724.dl.dropboxusercontent.com (uc3411270331445d0c4fb2c3b724.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n","Connecting to uc3411270331445d0c4fb2c3b724.dl.dropboxusercontent.com (uc3411270331445d0c4fb2c3b724.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /cd/0/inline2/BNUpl785dq6yOcZZm65z6blov8ElQAPuiHah6LlGoF6CpTfWH-4S5q6buXXtUuYFXh366FTW_Ld3heuNQ8UyEcRfgfoKKxpgeVlXUykWb7_hgotKtymnzIHgKSKkV-4pmSRmGUvaa5Bm6MyzzhiOGrlEeNLfWgNlEi8sFcV_7yUBvBzem-Ydj6-28p1wHqGc4a4maJSPivW7-kEFqDzNvK9iPndtJClkFkODg1Q7EGjTG4dUMOda9XmN8bjBZx7i2lDdlZRJ5WMxPRG1KlsQbcpxF4fYM2-jBT9KWXtplvD6ALipmQ8id7ItQDw3uhUTC1_wJbPCZi3EeUb7am8JU6X0lO3ZyanLei7kiR36rrYNEEl5ihk3G6gjWBcVgA_2aew/file [following]\n","--2021-04-26 12:05:07--  https://uc3411270331445d0c4fb2c3b724.dl.dropboxusercontent.com/cd/0/inline2/BNUpl785dq6yOcZZm65z6blov8ElQAPuiHah6LlGoF6CpTfWH-4S5q6buXXtUuYFXh366FTW_Ld3heuNQ8UyEcRfgfoKKxpgeVlXUykWb7_hgotKtymnzIHgKSKkV-4pmSRmGUvaa5Bm6MyzzhiOGrlEeNLfWgNlEi8sFcV_7yUBvBzem-Ydj6-28p1wHqGc4a4maJSPivW7-kEFqDzNvK9iPndtJClkFkODg1Q7EGjTG4dUMOda9XmN8bjBZx7i2lDdlZRJ5WMxPRG1KlsQbcpxF4fYM2-jBT9KWXtplvD6ALipmQ8id7ItQDw3uhUTC1_wJbPCZi3EeUb7am8JU6X0lO3ZyanLei7kiR36rrYNEEl5ihk3G6gjWBcVgA_2aew/file\n","Reusing existing connection to uc3411270331445d0c4fb2c3b724.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 452069309 (431M) [application/octet-stream]\n","Saving to: ‘./crypko_data.zip’\n","\n","./crypko_data.zip   100%[===================>] 431.13M   131MB/s    in 3.3s    \n","\n","2021-04-26 12:05:10 (131 MB/s) - ‘./crypko_data.zip’ saved [452069309/452069309]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pNtT1WCOyRNt"},"source":["###Unzip the downloaded file.\n","The unzipped tree structure is like \n","```\n","faces/\n","├── 1.jpg\n","├── 2.jpg\n","├── 3.jpg\n","...\n","```"]},{"cell_type":"code","metadata":{"id":"s2qR-0hjqWE6"},"source":["!unzip -q \"{workspace_dir}/crypko_data.zip\" -d \"{workspace_dir}/\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NjfM46dtmxXj"},"source":["## Random seed\n","Set the random seed to a certain value for reproducibility."]},{"cell_type":"code","metadata":{"id":"OWuecW1imz42"},"source":["import random\n","\n","import torch\n","import numpy as np\n","\n","\n","def same_seeds(seed):\n","    # Python built-in random module\n","    random.seed(seed)\n","    # Numpy\n","    np.random.seed(seed)\n","    # Torch\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","same_seeds(2021)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uCTPz2iRQmwe"},"source":["## Import Packages\n","First, we need to import packages that will be used later.\n","\n","Like hw3, we highly rely on **torchvision**, a library of PyTorch."]},{"cell_type":"code","metadata":{"id":"TC8RRsX0QhL-"},"source":["import os\n","import glob\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch import optim\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","from qqdm.notebook import qqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kYjZ_G83_YX4"},"source":["## Dataset\n","1. Resize the images to (64, 64)\n","1. Linearly map the values from [0, 1] to  [-1, 1].\n","\n","Please refer to [PyTorch official website](https://pytorch.org/vision/stable/transforms.html) for details about different transforms.\n"]},{"cell_type":"code","metadata":{"id":"UZ6d0_cr8R26"},"source":["class CrypkoDataset(Dataset):\n","    def __init__(self, fnames, transform):\n","        self.transform = transform\n","        self.fnames = fnames\n","        self.num_samples = len(self.fnames)\n","\n","    def __getitem__(self,idx):\n","        fname = self.fnames[idx]\n","        # 1. Load the image\n","        img = torchvision.io.read_image(fname)\n","        # 2. Resize and normalize the images using torchvision.\n","        img = self.transform(img)\n","        return img\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","\n","def get_dataset(root):\n","    fnames = glob.glob(os.path.join(root, '*'))\n","    # 1. Resize the image to (64, 64)\n","    # 2. Linearly map [0, 1] to [-1, 1]\n","    compose = [\n","        transforms.ToPILImage(),\n","        transforms.Resize((64, 64)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n","    ]\n","    transform = transforms.Compose(compose)\n","    dataset = CrypkoDataset(fnames, transform)\n","    return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tGwdVhOKSjLY"},"source":["### Show some images\n","Note that the values are in the range of [-1, 1], we should shift them to the valid range, [0, 1], to display correctly."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"id":"34mVNtHn7cwF","executionInfo":{"status":"error","timestamp":1619420268497,"user_tz":-480,"elapsed":10352,"user":{"displayName":"劉德駿","photoUrl":"","userId":"10755332246774080837"}},"outputId":"17d54232-6531-4a25-d654-fe34556720a9"},"source":["dataset = get_dataset(os.path.join(workspace_dir, 'faces'))\n","\n","images = [dataset[i] for i in range(16)]\n","grid_img = torchvision.utils.make_grid(images, nrow=4)\n","plt.figure(figsize=(10,10))\n","plt.imshow(grid_img.permute(1, 2, 0))\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-f6367049f61b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'faces'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgrid_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-f6367049f61b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'faces'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mgrid_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-5365688db78d>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# 1. Load the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","metadata":{"id":"nhxUjRUuHdti"},"source":["images = [(dataset[i]+1)/2 for i in range(16)]\n","grid_img = torchvision.utils.make_grid(images, nrow=4)\n","plt.figure(figsize=(10,10))\n","plt.imshow(grid_img.permute(1, 2, 0))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XkvZ4JgCHCZD"},"source":["## Model\n","Here, we use DCGAN as the model structure. Feel free to modify your own model structure.\n","\n","Note that the `N` of the input/output shape stands for the batch size."]},{"cell_type":"code","metadata":{"id":"F0I1jRd6HFmm"},"source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","    elif classname.find('BatchNorm') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n","\n","class Generator(nn.Module):\n","    \"\"\"\n","    Input shape: (N, in_dim)\n","    Output shape: (N, 3, 64, 64)\n","    \"\"\"\n","    def __init__(self, in_dim, dim=64):\n","        super(Generator, self).__init__()\n","        def dconv_bn_relu(in_dim, out_dim):\n","            return nn.Sequential(\n","                nn.ConvTranspose2d(in_dim, out_dim, 5, 2,\n","                                   padding=2, output_padding=1, bias=False),\n","                nn.BatchNorm2d(out_dim),\n","                nn.ReLU()\n","            )\n","        self.l1 = nn.Sequential(\n","            nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),\n","            nn.BatchNorm1d(dim * 8 * 4 * 4),\n","            nn.ReLU()\n","        )\n","        self.l2_5 = nn.Sequential(\n","            dconv_bn_relu(dim * 8, dim * 4),\n","            dconv_bn_relu(dim * 4, dim * 2),\n","            dconv_bn_relu(dim * 2, dim),\n","            nn.ConvTranspose2d(dim, 3, 5, 2, padding=2, output_padding=1),\n","            nn.Tanh()\n","        )\n","        self.apply(weights_init)\n","\n","    def forward(self, x):\n","        y = self.l1(x)\n","        y = y.view(y.size(0), -1, 4, 4)\n","        y = self.l2_5(y)\n","        return y\n","\n","\n","class Discriminator(nn.Module):\n","    \"\"\"\n","    Input shape: (N, 3, 64, 64)\n","    Output shape: (N, )\n","    \"\"\"\n","    def __init__(self, in_dim, dim=64):\n","        super(Discriminator, self).__init__()\n","\n","        def conv_bn_lrelu(in_dim, out_dim):\n","            return nn.Sequential(\n","                nn.Conv2d(in_dim, out_dim, 5, 2, 2),\n","                nn.BatchNorm2d(out_dim),\n","                nn.LeakyReLU(0.2),\n","            )\n","            \n","        \"\"\" Medium: Remove the last sigmoid layer for WGAN. \"\"\"\n","        self.ls = nn.Sequential(\n","            nn.Conv2d(in_dim, dim, 5, 2, 2), \n","            nn.LeakyReLU(0.2),\n","            conv_bn_lrelu(dim, dim * 2),\n","            conv_bn_lrelu(dim * 2, dim * 4),\n","            conv_bn_lrelu(dim * 4, dim * 8),\n","            nn.Conv2d(dim * 8, 1, 4),\n","            # nn.Sigmoid(), \n","        )\n","        self.apply(weights_init)\n","        \n","    def forward(self, x):\n","        y = self.ls(x)\n","        y = y.view(-1)\n","        return y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cxo4teqaO5RJ"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"v5sCGIUtSViC"},"source":["### Initialization\n","- hyperparameters\n","- model\n","- optimizer\n","- dataloader"]},{"cell_type":"code","metadata":{"id":"2EqomOouHezf"},"source":["# Training hyperparameters\n","batch_size = 64\n","z_dim = 100\n","z_sample = Variable(torch.randn(100, z_dim)).cuda()\n","lr = 1e-4\n","\n","\"\"\" Medium: WGAN, 50 epoch, n_critic=5, clip_value=0.01 \"\"\"\n","n_epoch = 50\n","n_critic = 5\n","clip_value = 0.01\n","\n","log_dir = os.path.join(workspace_dir, 'logs')\n","ckpt_dir = os.path.join(workspace_dir, 'checkpoints') # 這邊可以改gdrive\n","os.makedirs(log_dir, exist_ok=True)\n","os.makedirs(ckpt_dir, exist_ok=True)\n","\n","# Model\n","G = Generator(in_dim=z_dim).cuda()\n","D = Discriminator(3).cuda()\n","# G.load_state_dict(torch.load(os.path.join(ckpt_dir, 'G.pth')))\n","# D.load_state_dict(torch.load(os.path.join(ckpt_dir, 'D.pth')))\n","G.train()\n","D.train()\n","\n","# Loss\n","criterion = nn.BCELoss()\n","\n","\"\"\" Medium: Use RMSprop for WGAN. \"\"\"\n","# Optimizer\n","# opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n","# opt_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n","opt_D = torch.optim.RMSprop(D.parameters(), lr=lr)\n","opt_G = torch.optim.RMSprop(G.parameters(), lr=lr)\n","\n","\n","# DataLoader\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vpJA1wzi0tii"},"source":["### Training loop\n","We store some pictures regularly to monitor the current \bperformance of the Generator, and regularly record checkpoints."]},{"cell_type":"code","metadata":{"id":"dgkqPih1o5Az"},"source":["steps = 0\n","for e, epoch in enumerate(range(n_epoch)):\n","    progress_bar = qqdm(dataloader)\n","    for i, data in enumerate(progress_bar):\n","        imgs = data\n","        imgs = imgs.cuda()\n","\n","        bs = imgs.size(0)\n","\n","        # ============================================\n","        #  Train D\n","        # ============================================\n","        z = Variable(torch.randn(bs, z_dim)).cuda()\n","        r_imgs = Variable(imgs).cuda()\n","        f_imgs = G(z)\n","\n","        \"\"\" Medium: Use WGAN Loss. \"\"\"\n","        # # Label\n","        # r_label = torch.ones((bs)).cuda()\n","        # f_label = torch.zeros((bs)).cuda()\n","\n","        # # Model forwarding\n","        # r_logit = D(r_imgs.detach())\n","        # f_logit = D(f_imgs.detach())\n","        \n","        # # Compute the loss for the discriminator.\n","        # r_loss = criterion(r_logit, r_label)\n","        # f_loss = criterion(f_logit, f_label)\n","        # loss_D = (r_loss + f_loss) / 2\n","\n","        # WGAN Loss\n","        loss_D = -torch.mean(D(r_imgs)) + torch.mean(D(f_imgs))\n","       \n","\n","        # Model backwarding\n","        D.zero_grad()\n","        loss_D.backward()\n","\n","        # Update the discriminator.\n","        opt_D.step()\n","\n","        \"\"\" Medium: Clip weights of discriminator.(???) \"\"\"\n","        for p in D.parameters():\n","           p.data.clamp_(-clip_value, clip_value)\n","\n","        # ============================================\n","        #  Train G\n","        # ============================================\n","        if steps % n_critic == 0:\n","            # Generate some fake images.\n","            z = Variable(torch.randn(bs, z_dim)).cuda()\n","            f_imgs = G(z)\n","\n","            # Model forwarding\n","            f_logit = D(f_imgs)\n","            \n","            \"\"\" Medium: Use WGAN Loss\"\"\"\n","            # Compute the loss for the generator.\n","            # loss_G = criterion(f_logit, r_label)\n","            # WGAN Loss\n","            loss_G = -torch.mean(D(f_imgs))\n","\n","            # Model backwarding\n","            G.zero_grad()\n","            loss_G.backward()\n","\n","            # Update the generator.\n","            opt_G.step()\n","\n","        steps += 1\n","        \n","        # Set the info of the progress bar\n","        #   Note that the value of the GAN loss is not directly related to\n","        #   the quality of the generated images.\n","        progress_bar.set_infos({\n","            'Loss_D': round(loss_D.item(), 4),\n","            'Loss_G': round(loss_G.item(), 4),\n","            'Epoch': e+1,\n","            'Step': steps,\n","        })\n","\n","    G.eval()\n","    f_imgs_sample = (G(z_sample).data + 1) / 2.0\n","    filename = os.path.join(log_dir, f'Epoch_{epoch+1:03d}.jpg')\n","    torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n","    print(f' | Save some samples to {filename}.')\n","    \n","    # Show generated images in the jupyter notebook.\n","    grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n","    plt.figure(figsize=(10,10))\n","    plt.imshow(grid_img.permute(1, 2, 0))\n","    plt.show()\n","    G.train()\n","\n","    if (e+1) % 5 == 0 or e == 0:\n","        # Save the checkpoints.\n","        torch.save(G.state_dict(), os.path.join(ckpt_dir, 'G.pth'))\n","        torch.save(D.state_dict(), os.path.join(ckpt_dir, 'D.pth'))\n","'''記得optimizer也要存下來！ '''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e2uJFmTtKBeH"},"source":["## Inference\n","Use the trained model to generate anime faces!"]},{"cell_type":"markdown","metadata":{"id":"tXPXcVD_HJB2"},"source":["### Load model "]},{"cell_type":"code","metadata":{"id":"4JnQdNx2SUS2"},"source":["import torch\n","\n","G = Generator(z_dim)\n","G.load_state_dict(torch.load(os.path.join(ckpt_dir, 'G.pth')))\n","G.eval()\n","G.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-I8PDocbHQiN"},"source":["### Generate and show some images.\n"]},{"cell_type":"code","metadata":{"id":"x-SYKrRea_-Q"},"source":["# Generate 1000 images and make a grid to save them.\n","n_output = 1000\n","z_sample = Variable(torch.randn(n_output, z_dim)).cuda()\n","imgs_sample = (G(z_sample).data + 1) / 2.0\n","log_dir = os.path.join(workspace_dir, 'logs')\n","filename = os.path.join(log_dir, 'result.jpg')\n","torchvision.utils.save_image(imgs_sample, filename, nrow=10)\n","\n","# Show 32 of the images.\n","grid_img = torchvision.utils.make_grid(imgs_sample[:32].cpu(), nrow=10)\n","plt.figure(figsize=(10,10))\n","plt.imshow(grid_img.permute(1, 2, 0))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6B04ATOTHc4F"},"source":["### Compress the generated images using **tar**.\n"]},{"cell_type":"code","metadata":{"id":"mbcmoTQpz_yf"},"source":["# Save the generated images.\n","os.makedirs('output', exist_ok=True)\n","for i in range(1000):\n","    torchvision.utils.save_image(imgs_sample[i], f'output/{i+1}.jpg')\n","  \n","# Compress the images.\n","%cd output\n","!tar -zcf ../images.tgz *.jpg\n","%cd .."],"execution_count":null,"outputs":[]}]}